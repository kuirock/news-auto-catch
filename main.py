import os
import time
import re
from bs4 import BeautifulSoup
from supabase import create_client, Client

# --- ğŸ¤– Seleniumé–¢ä¿‚ ---
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- è¨­å®š ---
TARGET_URL = "https://portal.do-johodai.ac.jp/articles"
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
PORTAL_ID = os.environ.get("PORTAL_ID")
PORTAL_PASSWORD = os.environ.get("PORTAL_PASSWORD")

def setup_driver():
    print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ä¸­...")
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def login_and_scrape():
    if not SUPABASE_URL or not SUPABASE_KEY or not PORTAL_ID or not PORTAL_PASSWORD:
        print("âŒ è¨­å®šä¸è¶³: Secrets (URL, KEY, ID, PASSWORD) ã‚’ç¢ºèªã—ã¦ã­")
        return

    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    driver = setup_driver()
    
    # æ—¥ä»˜è¨˜éŒ²ç”¨ (UTC)
    from datetime import datetime, timezone
    current_run_time = datetime.now(timezone.utc).isoformat()
    
    try:
        # --- 1. ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç† ---
        print(f"ğŸ”— ãƒãƒ¼ã‚¿ãƒ«({TARGET_URL})ã«ã‚¢ã‚¯ã‚»ã‚¹...")
        driver.get(TARGET_URL)
        
        # ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢åˆ¤å®š
        if "login" in driver.current_url or "kc.do-johodai" in driver.current_url or "sso" in driver.current_url:
            print("ğŸ”’ ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã‚’æ¤œçŸ¥ï¼è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™...")
            wait = WebDriverWait(driver, 15)
            
            # IDå…¥åŠ›
            username_input = wait.until(EC.element_to_be_clickable((By.XPATH, "//input[@placeholder='ãƒ¦ãƒ¼ã‚¶ãƒ¼å' or @name='username' or @name='j_username']")))
            username_input.clear()
            username_input.send_keys(PORTAL_ID)
            
            # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
            password_input = driver.find_element(By.XPATH, "//input[@placeholder='ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰' or @name='password' or @name='j_password']")
            password_input.clear()
            password_input.send_keys(PORTAL_PASSWORD)
            
            # ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³
            try:
                login_btn = driver.find_element(By.XPATH, "//button[contains(text(), 'ãƒ­ã‚°ã‚¤ãƒ³') or @type='submit']")
                login_btn.click()
            except:
                password_input.submit()
            
            # é·ç§»å¾…ã¡
            print("â³ ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†ä¸­...")
            time.sleep(10)
        
        # --- 2. ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ«ãƒ¼ãƒ— ---
        page = 1
        total_count = 0
        is_success = True

        while True:
            # ãƒšãƒ¼ã‚¸ç§»å‹• (1ãƒšãƒ¼ã‚¸ç›®ã¯æ—¢ã«é–‹ã„ã¦ã„ã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚‹ã‘ã©å¿µã®ãŸã‚)
            if page > 1 or "page=" not in driver.current_url:
                current_page_url = f"{TARGET_URL}?page={page}"
                print(f"ğŸ“„ Page {page} ã¸ç§»å‹•ä¸­... ({current_page_url})")
                driver.get(current_page_url)
                time.sleep(3) # èª­ã¿è¾¼ã¿å¾…ã¡

            # HTMLè§£æ
            soup = BeautifulSoup(driver.page_source, "html.parser")
            
            # ã‚«ãƒ¼ãƒ‰å–å¾—
            cards = soup.find_all("div", class_="card-outline")
            
            if not cards:
                print(f"âœ… Page {page}: è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å–å¾—çµ‚äº†ï¼")
                break

            page_items = []
            for card in cards:
                try:
                    category_tag = card.find("span", class_="badge")
                    category = category_tag.get_text(strip=True) if category_tag else "ãŠçŸ¥ã‚‰ã›"
                    
                    h3_tag = card.find("h3", class_="card-title")
                    if not h3_tag: continue
                    full_text = h3_tag.get_text(strip=True)
                    
                    # æ—¥ä»˜æŠ½å‡º [2024/01/01] å½¢å¼
                    date_match = re.search(r'\[(\d{4}/\d{2}/\d{2})\]', full_text)
                    if date_match:
                        published_at = date_match.group(1).replace("/", "-")
                        title = full_text.replace(category, "").replace(date_match.group(0), "").strip()
                    else:
                        published_at = "2026-01-01" # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                        title = full_text.replace(category, "").strip()

                    footer = card.find("div", class_="card-footer")
                    link_tag = footer.find("a") if footer else None
                    if link_tag:
                        url = link_tag.get("href")
                        if url and not url.startswith("http"):
                            url = "https://portal.do-johodai.ac.jp" + url
                        
                        page_items.append({
                            "published_at": published_at,
                            "title": title,
                            "url": url,
                            "category": category,
                            "last_seen_at": current_run_time
                        })
                except Exception as e:
                    print(f"âš ï¸ è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            if not page_items:
                print("âš ï¸ ã‚«ãƒ¼ãƒ‰ã¯ã‚ã‚‹ã‘ã©ãƒ‡ãƒ¼ã‚¿ãŒå–ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
                break

            # DBä¿å­˜
            for item in page_items:
                supabase.table("news").upsert(item, on_conflict="url").execute()
            
            print(f"ğŸ’¾ Page {page}: {len(page_items)}ä»¶ ä¿å­˜å®Œäº†")
            total_count += len(page_items)
            page += 1

        # --- 3. ãŠæƒé™¤æ©Ÿèƒ½ ---
        if is_success and total_count > 0:
            print("ğŸ§¹ å¤ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãŠæƒé™¤ã‚’é–‹å§‹...")
            # ä»Šå›ã®å®Ÿè¡Œã§æ›´æ–°ã•ã‚Œãªã‹ã£ãŸ(last_seen_atãŒå¤ã„)ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
            result = supabase.table("news").delete().neq("last_seen_at", current_run_time).execute()
            print(f"âœ¨ ãŠæƒé™¤å®Œäº†ï¼å‰Šé™¤ã•ã‚ŒãŸä»¶æ•°: {len(result.data) if result.data else 'ä¸æ˜'}")
        else:
            print("âš ï¸ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒ1ä»¶ã‚‚å–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€å®‰å…¨ã®ãŸã‚å‰Šé™¤å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"âŒ å…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        driver.quit()
        print("ğŸ‘‹ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã¾ã—ãŸ")

if __name__ == "__main__":
    login_and_scrape()