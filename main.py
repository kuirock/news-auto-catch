import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
import time # ğŸ‘ˆ ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ãã™ã‚‹ãŸã‚ã«è¿½åŠ 

# --- 1. è¨­å®š ---
# ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ä¸€è¦§ã®ãƒ™ãƒ¼ã‚¹URL
BASE_URL = "https://portal.do-johodai.ac.jp/articles"

# ç’°å¢ƒå¤‰æ•°
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
PORTAL_COOKIE = os.environ.get("PORTAL_COOKIE")

def main():
    if not SUPABASE_URL or not SUPABASE_KEY or not PORTAL_COOKIE:
        print("è¨­å®šãŒè¶³ã‚Šãªã„ã‚ˆï¼Secretsã‚’ç¢ºèªã—ã¦ã­ã€‚")
        return

    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

    # Cookieã®æº–å‚™
    cookies = {}
    for item in PORTAL_COOKIE.split(";"):
        if "=" in item:
            key, value = item.strip().split("=", 1)
            cookies[key] = value

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    page = 1 # ğŸ‘ˆ 1ãƒšãƒ¼ã‚¸ç›®ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆï¼
    total_count = 0

    while True: # ğŸ‘ˆ çµ‚ã‚ã‚‹ã¾ã§ç„¡é™ãƒ«ãƒ¼ãƒ—ï¼
        current_url = f"{BASE_URL}?page={page}"
        print(f"--- ğŸ“„ Page {page} ã‚’è§£æä¸­... ---")
        
        try:
            response = requests.get(current_url, headers=headers, cookies=cookies, timeout=20)
            if response.status_code != 200:
                print(f"ã“ã‚Œä»¥ä¸Šãƒšãƒ¼ã‚¸ãŒãªã„ã‹ã€ã‚¨ãƒ©ãƒ¼ã ã‚ˆã€‚çµ‚äº†ã—ã¾ã™ã€‚ (Status: {response.status_code})")
                break
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            news_table = soup.find("table", class_="table")
            if not news_table:
                print("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªããªã£ãŸã‚ˆã€‚å…¨ä»¶å–å¾—å®Œäº†ï¼")
                break

            rows = news_table.find_all("tr")
            page_items = []

            for row in rows:
                cols = row.find_all("td")
                if len(cols) < 3: continue

                date_text = cols[0].text.strip().replace("/", "-")
                category = cols[1].find("span", class_="badge").text.strip() if cols[1].find("span") else "ãŠçŸ¥ã‚‰ã›"
                link_tag = cols[2].find("a")
                
                if link_tag:
                    title = link_tag.text.strip()
                    url = link_tag.get("href")
                    if url and not url.startswith("http"):
                        url = "https://portal.do-johodai.ac.jp" + url
                    
                    page_items.append({
                        "published_at": date_text,
                        "title": title,
                        "url": url,
                        "category": category
                    })

            if not page_items:
                print("ã“ã®ãƒšãƒ¼ã‚¸ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚‚ã†ãªã„ã¿ãŸã„ã€‚çµ‚äº†ï¼")
                break

            # Supabaseã«ä¿å­˜ï¼ˆ1ãƒšãƒ¼ã‚¸åˆ†ã¾ã¨ã‚ã¦ï¼‰
            for item in page_items:
                supabase.table("news").upsert(item, on_conflict="url").execute()
            
            print(f"Page {page}: {len(page_items)}ä»¶ä¿å­˜ã—ãŸã‚ˆï¼")
            total_count += len(page_items)

            # ğŸ›‘ ã‚µãƒ¼ãƒãƒ¼ã«è² è·ã‚’ã‹ã‘ãªã„ã‚ˆã†ã«1ç§’ä¼‘ã‚€ï¼ˆã“ã‚Œå¤§äº‹ï¼ï¼‰
            time.sleep(1)
            
            page += 1 # ğŸ‘ˆ æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸ï¼

        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            break

    print(f"âœ¨ å…¨ä½œæ¥­å®Œäº†ï¼ åˆè¨ˆ {total_count} ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åŒæœŸã—ãŸã‚ˆï¼")

if __name__ == "__main__":
    main()