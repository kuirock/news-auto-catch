import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client

# --- 1. è¨­å®š ---
# å¤§å­¦ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ãƒšãƒ¼ã‚¸ã®URL (ã“ã“ã‚’è‡ªåˆ†ã®å¤§å­¦ã®ã‚‚ã®ã«å¤‰ãˆã‚‹ï¼)
TARGET_URL = "https://www.do-johodai.ac.jp/news/" 

# Supabaseã®è¨­å®š (ã‚ã¨ã§GitHubã«ç™»éŒ²ã™ã‚‹ã‹ã‚‰ä»Šã¯ç©ºæ¬„ã§ã‚‚OK)
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

def main():
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("Supabaseã®éµãŒãªã„ã‚ˆï¼")
        return

    # Supabaseã«æ¥ç¶š
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

    # --- 2. å¤§å­¦ã‚µã‚¤ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ ---
    print(f"Fetching: {TARGET_URL}")
    response = requests.get(TARGET_URL)
    response.encoding = response.apparent_encoding # æ–‡å­—åŒ–ã‘å¯¾ç­–

    if response.status_code != 200:
        print("ã‚µã‚¤ãƒˆãŒé–‹ã‘ãªã‹ã£ãŸğŸ’¦")
        return

    # --- 3. HTMLã‚’è§£æ (ã“ã“ãŒå¤§å­¦ã«ã‚ˆã£ã¦é•ã†ï¼) ---
    soup = BeautifulSoup(response.text, "html.parser")
    
    # ğŸ‘‡ ã“ã“ãŒè¶…é‡è¦ï¼Chromeã®æ¤œè¨¼ãƒ„ãƒ¼ãƒ«ã§ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆãŒ
    #    ã©ã†ã„ã†ã‚¿ã‚°(ul, li, divãªã©)ã§æ›¸ã‹ã‚Œã¦ã„ã‚‹ã‹èª¿ã¹ã¦æ›¸ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚‹ã‚ˆï¼
    #    (ä»¥ä¸‹ã¯ã‚ãã¾ã§ã€Œã‚ˆãã‚ã‚‹ä¾‹ã€)
    news_items = []
    
    # ä¾‹: <ul class="news-list"> ã®ä¸­ã® <li> ã‚’æ¢ã™
    news_list = soup.find("ul", class_="news-list") 
    
    if not news_list:
        print("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ğŸ’¦ ã‚¿ã‚°ã®è¨­å®šã‚’å¤‰ãˆã¦ã¿ã¦ï¼")
        return

    for item in news_list.find_all("li"):
        try:
            # æ—¥ä»˜ã‚’å–å¾— (ä¾‹: <span class="date">2026.01.08</span>)
            date_text = item.find("span", class_="date").text.strip()
            
            # ãƒªãƒ³ã‚¯ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾— (ä¾‹: <a href="...">ã‚¿ã‚¤ãƒˆãƒ«</a>)
            link_tag = item.find("a")
            title = link_tag.text.strip()
            url = link_tag.get("href")
            
            # URLãŒç›¸å¯¾ãƒ‘ã‚¹(/news/...)ãªã‚‰çµ¶å¯¾ãƒ‘ã‚¹(https://...)ã«ã™ã‚‹
            if url.startswith("/"):
                url = "https://www.do-johodai.ac.jp" + url

            news_items.append({
                "published_at": date_text.replace(".", "-"), # 2026-01-08 ã®å½¢å¼ã«ã™ã‚‹
                "title": title,
                "url": url,
                "category": "ãŠçŸ¥ã‚‰ã›", # ã‚«ãƒ†ã‚´ãƒªãŒå–ã‚Œãªã‘ã‚Œã°å›ºå®šã§ã‚‚OK
            })
        except Exception as e:
            print(f"ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ: {e}")
            continue

    # --- 4. Supabaseã«ä¿å­˜ ---
    print(f"{len(news_items)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¦‹ã¤ã‘ãŸã‚ˆï¼ä¿å­˜ã™ã‚‹ã­...")
    
    for news in news_items:
        try:
            # upsert = ã‚ã‚Œã°æ›´æ–°ã€ãªã‘ã‚Œã°æ–°è¦è¿½åŠ 
            supabase.table("news").upsert(news, on_conflict="url").execute()
        except Exception as e:
            print(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    print("å®Œäº†ï¼ğŸ‰")

if __name__ == "__main__":
    main()