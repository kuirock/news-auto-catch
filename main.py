import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
import datetime

# --- 1. è¨­å®š ---
# ãƒãƒ¼ã‚¿ãƒ«ã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ (ã“ã“ã«ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚‹)
TARGET_URL = "https://portal.do-johodai.ac.jp/"

# ç’°å¢ƒå¤‰æ•° (GitHub Secretsã‹ã‚‰èª­ã¿è¾¼ã‚€)
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
PORTAL_COOKIE = os.environ.get("PORTAL_COOKIE")

def main():
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("Supabaseã®éµãŒãªã„ã‚ˆï¼")
        return
    if not PORTAL_COOKIE:
        print("CookieãŒãªã„ã‚ˆï¼")
        return

    # Supabaseã«æ¥ç¶š
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

    # --- 2. Cookieã‚’ä½¿ã£ã¦ã‚¢ã‚¯ã‚»ã‚¹ ---
    cookies = {}
    try:
        for item in PORTAL_COOKIE.split(";"):
            if "=" in item:
                key, value = item.strip().split("=", 1)
                cookies[key] = value
    except Exception as e:
        print(f"Cookieå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    print(f"Fetching: {TARGET_URL} ...")
    try:
        response = requests.get(TARGET_URL, headers=headers, cookies=cookies, timeout=20)
        response.encoding = response.apparent_encoding
        
        if response.status_code != 200:
            print(f"ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—ğŸ’¦ Status: {response.status_code}")
            return
    except Exception as e:
        print(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return

    # --- 3. HTMLã‚’è§£æ (ãƒãƒ¼ã‚¿ãƒ«ã‚µã‚¤ãƒˆä»•æ§˜) ---
    soup = BeautifulSoup(response.text, "html.parser")
    news_items = []

    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
    # ã‚¯ãƒ©ã‚¹å "table" ã‚’æŒã¤ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
    news_table = soup.find("table", class_="table")

    if not news_table:
        print("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ğŸ’¦ ãƒ­ã‚°ã‚¤ãƒ³ã§ãã¦ã‚‹ã‹ãªï¼Ÿ")
        # å¿µã®ãŸã‚ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤ºã—ã¦ç¢ºèª
        print(f"ç¾åœ¨ã®ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {soup.title.string if soup.title else 'ä¸æ˜'}")
        return

    # ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œ(tr)ã‚’å…¨éƒ¨å–å¾—
    rows = news_table.find_all("tr")
    print(f"{len(rows)} è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ç™ºè¦‹ï¼è§£æã™ã‚‹ã‚ˆ...")

    for row in rows:
        try:
            # å„è¡Œã®ã‚»ãƒ«(td)ã‚’å–å¾—
            cols = row.find_all("td")
            
            # ãƒ‡ãƒ¼ã‚¿ãŒå…¥ã£ã¦ã„ãªã„è¡Œï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ãªã©ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(cols) < 3:
                continue

            # 1åˆ—ç›®: æ—¥ä»˜ (ä¾‹: 2026/01/07)
            date_text = cols[0].text.strip()
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç”¨ã« / ã‚’ - ã«å¤‰æ› (2026-01-07)
            published_at = date_text.replace("/", "-")

            # 2åˆ—ç›®: ã‚«ãƒ†ã‚´ãƒª (ä¾‹: æƒ…å ±ã‚»ãƒ³ã‚¿ãƒ¼)
            # badgeã‚¯ãƒ©ã‚¹ã®ä¸­ã«ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹
            category_span = cols[1].find("span", class_="badge")
            category = category_span.text.strip() if category_span else "ãŠçŸ¥ã‚‰ã›"

            # 3åˆ—ç›®: ã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒªãƒ³ã‚¯
            link_tag = cols[2].find("a")
            if not link_tag:
                continue
                
            title = link_tag.text.strip()
            url = link_tag.get("href")

            # URLãŒç›¸å¯¾ãƒ‘ã‚¹ãªã‚‰çµ¶å¯¾ãƒ‘ã‚¹ã«ç›´ã™
            if url and not url.startswith("http"):
                 # ã‚‚ã— /articles/... ãªã‚‰ https://portal.do-johodai.ac.jp/articles/... ã«ã™ã‚‹
                url = "https://portal.do-johodai.ac.jp" + url

            # ä¿å­˜ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            news_data = {
                "published_at": published_at,
                "title": title,
                "url": url,
                "category": category,
            }
            
            news_items.append(news_data)
            print(f"å–å¾—: {published_at} [{category}] {title[:15]}...")

        except Exception as e:
            print(f"è¡Œã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
            continue

    # --- 4. Supabaseã«ä¿å­˜ ---
    if not news_items:
        print("ä¿å­˜ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªã‹ã£ãŸã‚ˆğŸ’¦")
        return

    print(f"{len(news_items)} ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¿å­˜é–‹å§‹ï¼")
    
    count = 0
    for news in news_items:
        try:
            # URLã‚’ã‚­ãƒ¼ã«ã—ã¦ã€ã™ã§ã«ã‚ã£ãŸã‚‰æ›´æ–°ã€ãªã‘ã‚Œã°è¿½åŠ 
            supabase.table("news").upsert(news, on_conflict="url").execute()
            count += 1
        except Exception as e:
            print(f"ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    print(f"å®Œäº†ï¼ {count} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ãŸã‚ˆï¼ğŸ‰")

if __name__ == "__main__":
    main()