import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
import time
import re # ğŸ‘ˆ æ—¥ä»˜ã‚’æŠœãå‡ºã™ãŸã‚ã«è¿½åŠ 

# --- 1. è¨­å®š ---
BASE_URL = "https://portal.do-johodai.ac.jp/articles"

SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
PORTAL_COOKIE = os.environ.get("PORTAL_COOKIE")

def main():
    if not SUPABASE_URL or not SUPABASE_KEY or not PORTAL_COOKIE:
        print("è¨­å®šãŒè¶³ã‚Šãªã„ã‚ˆï¼Secretsã‚’ç¢ºèªã—ã¦ã­ã€‚")
        return

    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

    cookies = {}
    for item in PORTAL_COOKIE.split(";"):
        if "=" in item:
            key, value = item.strip().split("=", 1)
            cookies[key] = value

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    page = 1
    total_count = 0

    while True:
        current_url = f"{BASE_URL}?page={page}"
        print(f"--- ğŸ“„ Page {page} ã‚’è§£æä¸­... ---")
        
        try:
            response = requests.get(current_url, headers=headers, cookies=cookies, timeout=20)
            response.encoding = response.apparent_encoding # æ–‡å­—åŒ–ã‘é˜²æ­¢
            
            if response.status_code != 200:
                print(f"ã“ã‚Œä»¥ä¸Šãƒšãƒ¼ã‚¸ãŒãªã„ã‹ã€ã‚¨ãƒ©ãƒ¼ã ã‚ˆã€‚çµ‚äº†ã—ã¾ã™ã€‚ (Status: {response.status_code})")
                break
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            # â˜… ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼šã‚«ãƒ¼ãƒ‰ï¼ˆdivï¼‰ã‚’å…¨éƒ¨æ¢ã™ â˜…
            cards = soup.find_all("div", class_="card-outline")
            
            if not cards:
                print("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªããªã£ãŸã‚ˆã€‚çµ‚äº†ï¼")
                break

            page_items = []

            for card in cards:
                try:
                    # 1. ã‚«ãƒ†ã‚´ãƒª
                    category_tag = card.find("span", class_="badge")
                    category = category_tag.get_text(strip=True) if category_tag else "ãŠçŸ¥ã‚‰ã›"

                    # 2. ã‚¿ã‚¤ãƒˆãƒ«ã¨æ—¥ä»˜ (h3ã‚¿ã‚°ã®ä¸­)
                    h3_tag = card.find("h3", class_="card-title")
                    if not h3_tag: continue
                    
                    full_text = h3_tag.get_text(strip=True)
                    # ã‚«ãƒ†ã‚´ãƒªåï¼ˆå­¦ç”ŸSCã¨ã‹ï¼‰ã‚’æ¶ˆã—ã¦ã€æ—¥ä»˜ã‚’æŠœãå‡ºã™
                    # ä¾‹: "å­¦ç”ŸSC ã€é‡è¦ã€‘æ—¥ç¨‹ã«ã¤ã„ã¦ [2026/01/08]"
                    
                    # æ—¥ä»˜ [YYYY/MM/DD] ã‚’æ¢ã™
                    date_match = re.search(r'\[(\d{4}/\d{2}/\d{2})\]', full_text)
                    if date_match:
                        published_at = date_match.group(1).replace("/", "-")
                        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªåã¨æ—¥ä»˜éƒ¨åˆ†ã‚’å‰Šã‚‹
                        title = full_text.replace(category, "").replace(date_match.group(0), "").strip()
                    else:
                        published_at = "2026-01-01" # å–ã‚Œãªã‹ã£ãŸæ™‚ã®äºˆå‚™
                        title = full_text.replace(category, "").strip()

                    # 3. URL
                    footer = card.find("div", class_="card-footer")
                    link_tag = footer.find("a") if footer else None
                    if link_tag:
                        url = link_tag.get("href")
                        if url and not url.startswith("http"):
                            url = "https://portal.do-johodai.ac.jp" + url
                        
                        page_items.append({
                            "published_at": published_at,
                            "title": title,
                            "url": url,
                            "category": category
                        })
                        print(f"å–å¾—: {published_at} [{category}] {title[:15]}...")

                except Exception as e:
                    print(f"å€‹åˆ¥ã‚«ãƒ¼ãƒ‰ã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            if not page_items:
                print("ã“ã®ãƒšãƒ¼ã‚¸ã«æœ‰åŠ¹ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒãªã„ã‚ˆã€‚")
                break

            # ä¿å­˜
            for item in page_items:
                supabase.table("news").upsert(item, on_conflict="url").execute()
            
            print(f"Page {page}: {len(page_items)}ä»¶ä¿å­˜å®Œäº†ï¼")
            total_count += len(page_items)

            # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ãã­ï¼
            time.sleep(1.5)
            page += 1

        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            break

    print(f"âœ¨ å…¨ä½œæ¥­å®Œäº†ï¼ åˆè¨ˆ {total_count} ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åŒæœŸã—ãŸã‚ˆï¼")

if __name__ == "__main__":
    main()