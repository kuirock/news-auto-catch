import os
import time
import re
from bs4 import BeautifulSoup
from supabase import create_client, Client

# --- ğŸ¤– Seleniumé–¢ä¿‚ ---
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# --- è¨­å®š ---
TARGET_URL = "https://portal.do-johodai.ac.jp/articles"
TOP_URL = "https://portal.do-johodai.ac.jp/top/"

SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
PORTAL_ID = os.environ.get("PORTAL_ID")
PORTAL_PASSWORD = os.environ.get("PORTAL_PASSWORD")
PORTAL_COOKIE = os.environ.get("PORTAL_COOKIE") # æ‰‹å‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨

def setup_driver():
    print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ä¸­...")
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1280,1024')
    # ãƒ­ãƒœãƒƒãƒˆæ¤œçŸ¥å›é¿
    options.add_argument('--disable-blink-features=AutomationControlled') 
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# â˜… DBã‹ã‚‰æœ€æ–°Cookieã‚’å–å¾—
def get_cookie_from_db(supabase):
    try:
        # system_cookiesãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ 'portal_session' ã¨ã„ã†ã‚­ãƒ¼ã‚’æ¢ã™
        res = supabase.table('system_cookies').select('value').eq('key', 'portal_session').execute()
        if res.data and len(res.data) > 0:
            print("ğŸ“¦ DBã‹ã‚‰æœ€æ–°ã®è‡ªå‹•æ›´æ–°CookieãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ã“ã‚Œã‚’ä½¿ã„ã¾ã™ã€‚")
            return res.data[0]['value']
    except Exception as e:
        print(f"âš ï¸ DBã‹ã‚‰ã®Cookieå–å¾—å¤±æ•— (åˆå›ã¯ãªãã¦OK): {e}")
    return None

# â˜… æœ€æ–°Cookieã‚’DBã«ä¿å­˜ï¼ˆã‚ã‚‰ã—ã¹é•·è€…ï¼‰
def save_cookie_to_db(driver, supabase):
    try:
        # Seleniumã‹ã‚‰Cookieãƒªã‚¹ãƒˆã‚’å–å¾—
        cookies = driver.get_cookies()
        # æ–‡å­—åˆ—ã«å¤‰æ›
        cookie_str = "; ".join([f"{c['name']}={c['value']}" for c in cookies])
        
        if not cookie_str:
            return

        # DBã«ä¸Šæ›¸ãä¿å­˜ (Upsert)
        supabase.table('system_cookies').upsert({
            'key': 'portal_session',
            'value': cookie_str,
            'updated_at': 'now()'
        }).execute()
        print("ğŸ’¾ æœ€æ–°ã®Cookieã‚’DBã«ä¿å­˜ã—ã¾ã—ãŸï¼æ¬¡å›ã®ãƒ­ãƒœãƒƒãƒˆã¯ã“ã‚Œã‚’ä½¿ã„ã¾ã™ã€‚")
    except Exception as e:
        print(f"âŒ Cookieã®ä¿å­˜ã«å¤±æ•—: {e}")

def inject_cookies(driver, cookie_str):
    if not cookie_str:
        return False
    
    print("ğŸª Cookieã®æ³¨å…¥ã‚’é–‹å§‹ã—ã¾ã™...")
    try:
        driver.get(TOP_URL) # ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’åˆã‚ã›ã‚‹ãŸã‚ã‚¢ã‚¯ã‚»ã‚¹
        
        cookies = cookie_str.split(';')
        for cookie in cookies:
            if '=' in cookie:
                name, value = cookie.strip().split('=', 1)
                driver.add_cookie({
                    'name': name.strip(),
                    'value': value.strip(),
                    'domain': 'portal.do-johodai.ac.jp',
                    'path': '/'
                })
        print("âœ… Cookieæ³¨å…¥å®Œäº†ï¼")
        return True
    except Exception as e:
        print(f"âŒ Cookieæ³¨å…¥å¤±æ•—: {e}")
        return False

def perform_google_login(driver, wait):
    print("ğŸ”’ Google SSOãƒ­ã‚°ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹...")

    # 1. ãƒãƒ¼ã‚¿ãƒ«ã®ã€Œãƒ­ã‚°ã‚¤ãƒ³ã€ãƒœã‚¿ãƒ³
    try:
        portal_login_btn = wait.until(EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'ãƒ­ã‚°ã‚¤ãƒ³')] | //a[contains(text(), 'ãƒ­ã‚°ã‚¤ãƒ³')]")))
        print("ğŸ‘† ãƒãƒ¼ã‚¿ãƒ«ã®ã€ãƒ­ã‚°ã‚¤ãƒ³ã€‘ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼")
        portal_login_btn.click()
    except TimeoutException:
        print("â„¹ï¸ ãƒãƒ¼ã‚¿ãƒ«ã®ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    # 2. ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹å…¥åŠ›
    try:
        print("ğŸ“§ Google: ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹å…¥åŠ›å¾…ã¡...")
        email_input = wait.until(EC.visibility_of_element_located((By.XPATH, "//input[@type='email']")))
        email_input.clear()
        email_input.send_keys(PORTAL_ID)
        time.sleep(0.5)
        email_input.send_keys(Keys.RETURN)
        print("âœ… ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹é€ä¿¡")
    except TimeoutException:
        print("â„¹ï¸ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹å…¥åŠ›æ¬„ãŒå‡ºã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")

    # 3. ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
    try:
        print("ğŸ”‘ Google: ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›å¾…ã¡...")
        password_input = wait.until(EC.visibility_of_element_located((By.XPATH, "//input[@type='password']")))
        time.sleep(1)
        password_input.clear()
        password_input.send_keys(PORTAL_PASSWORD)
        time.sleep(0.5)
        password_input.send_keys(Keys.RETURN)
        print("âœ… ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰é€ä¿¡")
    except TimeoutException:
        print("â„¹ï¸ ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›æ¬„ãŒå‡ºã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")

    print("â³ ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†å®Œäº†å¾…ã¡...")
    time.sleep(10)
    
    if "login" not in driver.current_url:
        print("ğŸ‰ ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸï¼")
        return True
    else:
        print(f"âš ï¸ ãƒ­ã‚°ã‚¤ãƒ³å¾Œã®URLãŒæ€ªã—ã„ã§ã™: {driver.current_url}")
        return False

def login_and_scrape():
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("âŒ Supabaseè¨­å®šä¸è¶³")
        return

    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    driver = setup_driver()
    
    # æ—¥ä»˜è¨˜éŒ²ç”¨ (UTC)
    from datetime import datetime, timezone
    current_run_time = datetime.now(timezone.utc).isoformat()
    
    try:
        wait = WebDriverWait(driver, 30)

        # --- 1. Cookieæˆ¦ç•¥ ---
        # å„ªå…ˆé †ä½: â‘ DBã®è‡ªå‹•æ›´æ–°Cookie -> â‘¡GitHub Secretsã®æ‰‹å‹•Cookie
        target_cookie = get_cookie_from_db(supabase)
        if not target_cookie:
            print("â„¹ï¸ DBã«CookieãŒãªã„ãŸã‚ã€æ‰‹å‹•Cookie(Secrets)ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            target_cookie = PORTAL_COOKIE

        # Cookieæ³¨å…¥
        inject_cookies(driver, target_cookie)
        
        # ã‚µã‚¤ãƒˆã¸ã‚¢ã‚¯ã‚»ã‚¹
        print(f"ğŸ”— ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§({TARGET_URL})ã¸ç§»å‹•...")
        driver.get(TARGET_URL)
        time.sleep(3)

        # ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—åˆ¤å®š
        current_url = driver.current_url
        is_login_page = "login" in current_url or "google" in current_url
        is_top_page = "/top/" in current_url 

        if is_login_page or is_top_page:
            print("âš ï¸ Cookieãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã€ã¾ãŸã¯æœ‰åŠ¹æœŸé™åˆ‡ã‚Œã§ã™ã€‚")
            
            if not PORTAL_ID or not PORTAL_PASSWORD:
                print("âŒ ID/PASSãŒãªã„ãŸã‚çµ‚äº†ã—ã¾ã™ã€‚")
                return

            print("ğŸ”„ é€šå¸¸ã®ãƒ­ã‚°ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œã—ã¾ã™...")
            perform_google_login(driver, wait)
            
            print("â†©ï¸ å†åº¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§ã¸ç§»å‹•...")
            driver.get(TARGET_URL)
            time.sleep(5)

        # â˜… ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸã—ãŸã‚‰ã€æ—©é€Ÿæœ€æ–°Cookieã‚’ä¿å­˜ã—ã¦ãŠã
        if "portal.do-johodai.ac.jp" in driver.current_url and "login" not in driver.current_url:
             save_cookie_to_db(driver, supabase)

        # --- 2. ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ãƒ«ãƒ¼ãƒ— ---
        page = 1
        total_count = 0
        is_success = True

        while True:
            # ãƒšãƒ¼ã‚¸ç§»å‹•
            if page > 1 or "articles" not in driver.current_url:
                driver.get(f"{TARGET_URL}?page={page}")
                time.sleep(2)
            
            try:
                # è¨˜äº‹ã‚«ãƒ¼ãƒ‰ãŒå‡ºã‚‹ã¾ã§å¾…ã¤
                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".card-outline, .card")))
                time.sleep(2)
            except TimeoutException:
                print(f"âš ï¸ è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (Page {page})")
                print(f"   URL: {driver.current_url}")
                # ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã«æˆ»ã•ã‚Œã¦ã„ãŸã‚‰çµ‚äº†
                if "login" in driver.current_url:
                    print("ğŸš¨ ãƒ­ã‚°ã‚¢ã‚¦ãƒˆã•ã‚Œã¾ã—ãŸã€‚")
                    is_success = False
                break

            soup = BeautifulSoup(driver.page_source, "html.parser")
            valid_cards = [c for c in soup.select(".card-outline, .card") if c.find("h3") or c.find("a")]

            if not valid_cards:
                print("âœ… ã“ã‚Œä»¥ä¸Šè¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                break

            page_items = []
            for card in valid_cards:
                try:
                    category_tag = card.find("span", class_="badge")
                    category = category_tag.get_text(strip=True) if category_tag else "ãŠçŸ¥ã‚‰ã›"
                    
                    h3_tag = card.find("h3", class_="card-title") or card.find("a")
                    if not h3_tag: continue
                    full_text = h3_tag.get_text(strip=True)
                    
                    date_match = re.search(r'\[(\d{4}/\d{2}/\d{2})\]', full_text)
                    if date_match:
                        published_at = date_match.group(1).replace("/", "-")
                        title = full_text.replace(category, "").replace(date_match.group(0), "").strip()
                    else:
                        published_at = "2026-01-01"
                        title = full_text.replace(category, "").strip()

                    footer = card.find("div", class_="card-footer")
                    link_tag = footer.find("a") if footer else card.find("a")

                    if link_tag:
                        url = link_tag.get("href")
                        if url and not url.startswith("http"):
                            url = "https://portal.do-johodai.ac.jp" + url
                        
                        page_items.append({
                            "published_at": published_at,
                            "title": title,
                            "url": url,
                            "category": category,
                            "last_seen_at": current_run_time
                        })
                except:
                    continue

            if not page_items: break

            for item in page_items:
                supabase.table("news").upsert(item, on_conflict="url").execute()
            
            print(f"ğŸ’¾ Page {page}: {len(page_items)}ä»¶ ä¿å­˜")
            total_count += len(page_items)
            page += 1

        # --- 3. ãŠæƒé™¤æ©Ÿèƒ½ ---
        if is_success and total_count > 0:
            print("ğŸ§¹ å¤ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãŠæƒé™¤ã‚’é–‹å§‹...")
            result = supabase.table("news").delete().neq("last_seen_at", current_run_time).execute()
            count = len(result.data) if result.data else 0
            print(f"âœ¨ ãŠæƒé™¤å®Œäº†ï¼å‰Šé™¤ã•ã‚ŒãŸä»¶æ•°: {count}")
            
            # æœ€å¾Œã«ãƒ€ãƒ¡æŠ¼ã—ã§æœ€æ–°Cookieä¿å­˜
            save_cookie_to_db(driver, supabase)
        else:
            print(f"âš ï¸ å–å¾—æ•°: {total_count}ã€‚å‰Šé™¤ã‚¹ã‚­ãƒƒãƒ—ã€‚")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        driver.quit()

if __name__ == "__main__":
    login_and_scrape()