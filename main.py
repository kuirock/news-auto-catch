import os
import time
import requests
import re
from bs4 import BeautifulSoup
from supabase import create_client, Client

# --- ğŸ¤– Seleniumé–¢ä¿‚ ---
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- 1. è¨­å®š ---
# æœ€åˆã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã®ã¯ãƒãƒ¼ã‚¿ãƒ«ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸€è¦§
TARGET_URL = "https://portal.do-johodai.ac.jp/articles"

SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
PORTAL_ID = os.environ.get("PORTAL_ID")
PORTAL_PASSWORD = os.environ.get("PORTAL_PASSWORD")

def get_fresh_cookie():
    print("ğŸ¤– ãƒ­ãƒœãƒƒãƒˆãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ä¸­...")
    
    options = Options()
    options.add_argument('--headless') 
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument(f"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    
    try:
        # 1. ã¾ãšãƒãƒ¼ã‚¿ãƒ«ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã«è¡Œã£ã¦ã¿ã‚‹
        print(f"ğŸ”— ãƒãƒ¼ã‚¿ãƒ«({TARGET_URL})ã«ã‚¢ã‚¯ã‚»ã‚¹...")
        driver.get(TARGET_URL)
        
        # 2. URLã‚’ãƒã‚§ãƒƒã‚¯ï¼ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã«é£›ã°ã•ã‚ŒãŸã‹ï¼Ÿ
        current_url = driver.current_url
        print(f"ğŸ“ ç¾åœ¨ã®URL: {current_url}")

        # ã‚‚ã—ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ï¼ˆkc.do-johodai... ã‚„ login...ï¼‰ã«ã„ãŸã‚‰ãƒ­ã‚°ã‚¤ãƒ³ã‚’è©¦ã¿ã‚‹
        if "login" in current_url or "kc.do-johodai" in current_url or "sso" in current_url:
            print("ğŸ”’ ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã‚’æ¤œçŸ¥ï¼è‡ªå‹•ãƒ­ã‚°ã‚¤ãƒ³ã‚’è©¦ã¿ã¾ã™...")
            
            wait = WebDriverWait(driver, 15)
            
            # --- å…¥åŠ›æ¬„ã‚’æ¢ã™ä½œæˆ¦ ---
            # ã‚¹ã‚¯ã‚·ãƒ§ã‚’è¦‹ãŸæ„Ÿã˜ã€placeholder="ãƒ¦ãƒ¼ã‚¶ãƒ¼å" ã¨ãªã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã®ã§
            # XPathã‚’ä½¿ã£ã¦ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ã„ã†æ–‡å­—ãŒå…¥ã£ã¦ã‚‹å…¥åŠ›æ¬„ã€ã‚’æ¢ã™ã‚ˆï¼
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼IDå…¥åŠ›
            try:
                username_input = wait.until(EC.element_to_be_clickable((By.XPATH, "//input[@placeholder='ãƒ¦ãƒ¼ã‚¶ãƒ¼å' or @name='username' or @name='j_username']")))
                username_input.clear()
                username_input.send_keys(PORTAL_ID)
                print("âœ… IDå…¥åŠ›å®Œäº†")
            except:
                print("âš ï¸ IDå…¥åŠ›æ¬„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚HTMLãŒå¤‰ã‚ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                raise

            # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
            try:
                password_input = driver.find_element(By.XPATH, "//input[@placeholder='ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰' or @name='password' or @name='j_password']")
                password_input.clear()
                password_input.send_keys(PORTAL_PASSWORD)
                print("âœ… ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›å®Œäº†")
            except:
                print("âš ï¸ ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›æ¬„ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                raise

            # ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ï¼ˆã€Œãƒ­ã‚°ã‚¤ãƒ³ã€ã¨ã„ã†æ–‡å­—ãŒå«ã¾ã‚Œã‚‹ãƒœã‚¿ãƒ³ã‚’æ¢ã™ï¼‰
            try:
                login_button = driver.find_element(By.XPATH, "//button[contains(text(), 'ãƒ­ã‚°ã‚¤ãƒ³') or @type='submit']")
                login_button.click()
                print("ğŸ‘† ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼")
            except:
                # ãƒœã‚¿ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€Enterã‚­ãƒ¼ã§é€ä¿¡ã—ã¦ã¿ã‚‹
                password_input.submit()
                print("ğŸ‘† (Enterã‚­ãƒ¼ã§é€ä¿¡)")

            # 3. ãƒ­ã‚°ã‚¤ãƒ³å¾Œã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’å¾…ã¤
            # ãƒãƒ¼ã‚¿ãƒ«ã®URLã«æˆ»ã£ã¦ãã‚‹ã¾ã§å¾…æ©Ÿ
            print("â³ ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†ä¸­...")
            time.sleep(10) # é·ç§»å¾…ã¡ï¼ˆé•·ã‚ã«ï¼‰

            print(f"ğŸ“ é·ç§»å¾Œã®URL: {driver.current_url}")
        
        # 4. Cookieã‚’å›å
        selenium_cookies = driver.get_cookies()
        cookie_dict = {}
        for cookie in selenium_cookies:
            cookie_dict[cookie['name']] = cookie['value']
            
        print("ğŸª æ–°é®®ãªCookieã‚’ã‚²ãƒƒãƒˆã—ã¾ã—ãŸï¼")
        return cookie_dict

    except Exception as e:
        print(f"ğŸ˜± ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—: {e}")
        return None
    finally:
        driver.quit()

def main():
    if not SUPABASE_URL or not SUPABASE_KEY or not PORTAL_ID or not PORTAL_PASSWORD:
        print("è¨­å®šãŒè¶³ã‚Šãªã„ã‚ˆï¼Secrets (URL, KEY, ID, PASSWORD) ã‚’ç¢ºèªã—ã¦ã­ã€‚")
        return

    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

    # ãƒ­ãƒœãƒƒãƒˆå‡ºå‹•ï¼
    cookies = get_fresh_cookie()
    
    if not cookies:
        print("ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ãŸã®ã§çµ‚äº†ã—ã¾ã™ğŸ˜­")
        return

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    # æ—¥ä»˜è¨˜éŒ²ç”¨
    from datetime import datetime, timezone
    current_run_time = datetime.now(timezone.utc).isoformat()

    page = 1
    total_count = 0
    is_success = True

    while True:
        current_url = f"{TARGET_URL}?page={page}"
        print(f"--- ğŸ“„ Page {page} ã‚’è§£æä¸­... ---")
        
        try:
            # ã•ã£ãã‚²ãƒƒãƒˆã—ãŸCookieã‚’ä½¿ã£ã¦ã‚¢ã‚¯ã‚»ã‚¹ï¼
            response = requests.get(current_url, headers=headers, cookies=cookies, timeout=20)
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ã¾ã ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã«ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆçªç ´å¤±æ•—ã®å¯èƒ½æ€§ï¼‰
            if soup.title and ("Login" in soup.title.string or "ãƒ­ã‚°ã‚¤ãƒ³" in soup.title.string):
                print("ğŸš¨ ã‚¨ãƒ©ãƒ¼: ã¾ã ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã«ã„ã¾ã™ã€‚çªç ´ã«å¤±æ•—ã—ã¾ã—ãŸğŸ’¦")
                is_success = False
                break

            cards = soup.find_all("div", class_="card-outline")
            
            if not cards:
                print("ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—å®Œäº†ï¼")
                break

            page_items = []
            for card in cards:
                try:
                    category_tag = card.find("span", class_="badge")
                    category = category_tag.get_text(strip=True) if category_tag else "ãŠçŸ¥ã‚‰ã›"
                    
                    h3_tag = card.find("h3", class_="card-title")
                    if not h3_tag: continue
                    full_text = h3_tag.get_text(strip=True)
                    
                    date_match = re.search(r'\[(\d{4}/\d{2}/\d{2})\]', full_text)
                    if date_match:
                        published_at = date_match.group(1).replace("/", "-")
                        title = full_text.replace(category, "").replace(date_match.group(0), "").strip()
                    else:
                        published_at = "2026-01-01"
                        title = full_text.replace(category, "").strip()

                    footer = card.find("div", class_="card-footer")
                    link_tag = footer.find("a") if footer else None
                    if link_tag:
                        url = link_tag.get("href")
                        if url and not url.startswith("http"):
                            url = "https://portal.do-johodai.ac.jp" + url
                        
                        page_items.append({
                            "published_at": published_at,
                            "title": title,
                            "url": url,
                            "category": category,
                            "last_seen_at": current_run_time
                        })
                except Exception as e:
                    continue

            if not page_items: break

            for item in page_items:
                supabase.table("news").upsert(item, on_conflict="url").execute()
            
            print(f"Page {page}: {len(page_items)}ä»¶ ä¿å­˜å®Œäº†")
            total_count += len(page_items)
            time.sleep(1)
            page += 1

        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: {e}")
            is_success = False
            break
            
    # ãŠæƒé™¤æ©Ÿèƒ½ï¼ˆã•ã£ãã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰
    if is_success and total_count > 0:
        try:
            supabase.table("news").delete().neq("last_seen_at", current_run_time).execute()
            print("ğŸ§¹ å¤ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãŠæƒé™¤å®Œäº†ï¼")
        except Exception as e:
            print(f"âš ï¸ ãŠæƒé™¤ã‚¨ãƒ©ãƒ¼: {e}")

    print(f"âœ¨ å‡¦ç†çµ‚äº†ï¼")

if __name__ == "__main__":
    main()