import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
import time
import re
from datetime import datetime, timezone # ğŸ‘ˆ æ—¥ä»˜ç”¨ã«è¿½åŠ 

# --- 1. è¨­å®š ---
BASE_URL = "https://portal.do-johodai.ac.jp/articles"

SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
PORTAL_COOKIE = os.environ.get("PORTAL_COOKIE")

def main():
    if not SUPABASE_URL or not SUPABASE_KEY or not PORTAL_COOKIE:
        print("è¨­å®šãŒè¶³ã‚Šãªã„ã‚ˆï¼Secretsã‚’ç¢ºèªã—ã¦ã­ã€‚")
        return

    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

    # â˜… ä»Šå›ã®å®Ÿè¡Œæ™‚åˆ»ã‚’ã€Œå‡ºå¸­ã‚¹ã‚¿ãƒ³ãƒ—ã€ã¨ã—ã¦ä½¿ã†ã‚ˆï¼
    # (ISOå½¢å¼ã®æ–‡å­—åˆ—ã«ã—ã¦ãŠã)
    current_run_time = datetime.now(timezone.utc).isoformat()
    print(f"ğŸ•’ ä»Šå›ã®å®Ÿè¡ŒID (last_seen_at): {current_run_time}")

    cookies = {}
    for item in PORTAL_COOKIE.split(";"):
        if "=" in item:
            key, value = item.strip().split("=", 1)
            cookies[key] = value

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    page = 1
    total_count = 0
    is_success = True # æœ€å¾Œã¾ã§å®Œäº†ã—ãŸã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ•ãƒ©ã‚°

    while True:
        current_url = f"{BASE_URL}?page={page}"
        print(f"--- ğŸ“„ Page {page} ã‚’è§£æä¸­... ---")
        
        try:
            response = requests.get(current_url, headers=headers, cookies=cookies, timeout=20)
            response.encoding = response.apparent_encoding 
            
            soup = BeautifulSoup(response.text, "html.parser")
            page_title = soup.title.string.strip() if soup.title else ""
            
            if "Login" in page_title or "ãƒ­ã‚°ã‚¤ãƒ³" in page_title or "Google" in page_title:
                print("ğŸš¨ ã‚¨ãƒ©ãƒ¼: ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã«è»¢é€ã•ã‚Œã¾ã—ãŸã€‚")
                is_success = False # å¤±æ•—ãƒ•ãƒ©ã‚°
                break

            cards = soup.find_all("div", class_="card-outline")
            
            if not cards:
                print("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆã“ã‚Œä»¥ä¸Šãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ï¼‰ã€‚")
                break

            page_items = []
            for card in cards:
                try:
                    category_tag = card.find("span", class_="badge")
                    category = category_tag.get_text(strip=True) if category_tag else "ãŠçŸ¥ã‚‰ã›"

                    h3_tag = card.find("h3", class_="card-title")
                    if not h3_tag: continue
                    full_text = h3_tag.get_text(strip=True)
                    
                    date_match = re.search(r'\[(\d{4}/\d{2}/\d{2})\]', full_text)
                    if date_match:
                        published_at = date_match.group(1).replace("/", "-")
                        title = full_text.replace(category, "").replace(date_match.group(0), "").strip()
                    else:
                        published_at = "2026-01-01"
                        title = full_text.replace(category, "").strip()

                    footer = card.find("div", class_="card-footer")
                    link_tag = footer.find("a") if footer else None
                    if link_tag:
                        url = link_tag.get("href")
                        if url and not url.startswith("http"):
                            url = "https://portal.do-johodai.ac.jp" + url
                        
                        page_items.append({
                            "published_at": published_at,
                            "title": title,
                            "url": url,
                            "category": category,
                            "last_seen_at": current_run_time # ğŸ‘ˆ ã“ã“ã§ã€Œå‡ºå¸­ã‚¹ã‚¿ãƒ³ãƒ—ã€ã‚’æŠ¼ã™ï¼
                        })

                except Exception as e:
                    print(f"è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            if not page_items: break

            for item in page_items:
                supabase.table("news").upsert(item, on_conflict="url").execute()
            
            print(f"Page {page}: {len(page_items)}ä»¶ä¿å­˜å®Œäº†")
            total_count += len(page_items)
            time.sleep(1)
            page += 1

        except Exception as e:
            print(f"é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            is_success = False # å¤±æ•—ãƒ•ãƒ©ã‚°
            break

    # --- ğŸ§¹ ãŠæƒé™¤ã‚¿ã‚¤ãƒ  ---
    # ã‚¨ãƒ©ãƒ¼ãªãæœ€å¾Œã¾ã§èµ°ã‚Šåˆ‡ã£ãŸå ´åˆã ã‘ã€å‰Šé™¤ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆå®‰å…¨ã®ãŸã‚ï¼‰
    if is_success and total_count > 0:
        print("ğŸ§¹ å‰Šé™¤ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ãŠæƒé™¤ã‚’é–‹å§‹ã—ã¾ã™...")
        try:
            # ã€Œä»Šå›ã®ã‚¹ã‚¿ãƒ³ãƒ—(current_run_time)ã‚’æŒã£ã¦ã„ãªã„ã€ï¼ã€Œä»Šå›è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã€ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
            result = supabase.table("news").delete().neq("last_seen_at", current_run_time).execute()
            # â€» neq ã¯ "Not Equal" (ç­‰ã—ããªã„) ã®æ„å‘³ã ã‚ˆ
            
            # æ¶ˆã—ãŸæ•°ã‚’ç¢ºèªï¼ˆdataãŒãƒªã‚¹ãƒˆã§è¿”ã£ã¦ãã‚‹ã¯ãšï¼‰
            deleted_count = len(result.data) if result.data else 0
            print(f"âœ¨ ãŠæƒé™¤å®Œäº†ï¼ {deleted_count} ä»¶ã®å¤ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
            
        except Exception as e:
            print(f"âš ï¸ ãŠæƒé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆãƒ‡ãƒ¼ã‚¿ã¯æ®‹ã‚Šã¾ã™ï¼‰: {e}")
    else:
        print("âš ï¸ é€”ä¸­ã§ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ãŸãŸã‚ã€å‰Šé™¤å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

    print(f"ğŸ å…¨å‡¦ç†çµ‚äº†ï¼")

if __name__ == "__main__":
    main()