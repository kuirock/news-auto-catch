import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client

# ãƒãƒ¼ã‚¿ãƒ«ã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒã‚ã‚‹å ´æ‰€ï¼‰
TARGET_URL = "https://portal.do-johodai.ac.jp/" 

# ç’°å¢ƒå¤‰æ•°
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")
PORTAL_COOKIE = os.environ.get("PORTAL_COOKIE") # ğŸ‘ˆ æ‰‹å½¢ã‚’å—ã‘å–ã‚‹

def main():
    if not PORTAL_COOKIE:
        print("CookieãŒãªã„ã‚ˆï¼GitHub Secretsã‚’è¨­å®šã—ã¦ã­ï¼")
        return

    # --- 1. æ‰‹å½¢ï¼ˆCookieï¼‰ã‚’ä½¿ã£ã¦ã‚¢ã‚¯ã‚»ã‚¹ ---
    # Cookieã¯ "key=value; key2=value2" ã¨ã„ã†æ–‡å­—åˆ—ãªã®ã§ã€è¾æ›¸å‹ã«å¤‰æ›ã™ã‚‹
    cookies = {}
    try:
        for item in PORTAL_COOKIE.split(";"):
            if "=" in item:
                key, value = item.strip().split("=", 1)
                cookies[key] = value
    except Exception as e:
        print(f"Cookieã®å¤‰æ›ã«å¤±æ•—: {e}")
        return

    print(f"Fetching: {TARGET_URL} with Cookies...")
    
    # User-Agentï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã®ãµã‚Šã‚’ã™ã‚‹ï¼‰
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    try:
        # cookiesã‚’æ¸¡ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹ï¼
        response = requests.get(TARGET_URL, headers=headers, cookies=cookies, timeout=20)
        response.encoding = response.apparent_encoding

        if response.status_code != 200:
            print(f"ã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—ğŸ’¦ Status: {response.status_code}")
            # ãƒ­ã‚°ã‚¤ãƒ³ç”»é¢ã«é£›ã°ã•ã‚Œã¦ã‚‹ã‹ã‚‚ï¼Ÿ
            print(f"URL: {response.url}")
            return

        # --- 2. æˆåŠŸã—ãŸã‚‰HTMLã‚’ç¢ºèª ---
        print("ã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸï¼ğŸ‰")
        
        # HTMLã®ä¸€éƒ¨ã‚’è¡¨ç¤ºã—ã¦ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒå«ã¾ã‚Œã¦ã‚‹ã‹ç¢ºèªã—ãŸã„
        # (ã“ã“ã‹ã‚‰ä¸‹ã¯ã€HTMLãŒè¦‹ã‚Œã¦ã‹ã‚‰æœ¬æ°—å‡ºã™ï¼)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤º
        print(f"Page Title: {soup.title.string if soup.title else 'No Title'}")

        # è©¦ã—ã«ã€ŒãŠçŸ¥ã‚‰ã›ã€ã£ã½ã„è¦ç´ ã‚’æ¢ã—ã¦ã¿ã‚‹ï¼ˆä»®ï¼‰
        # AdminLTEï¼ˆãƒãƒ¼ã‚¿ãƒ«ã®ãƒ‡ã‚¶ã‚¤ãƒ³ï¼‰ã«ã‚ˆãã‚ã‚‹æ§‹é€ ã‚’æ¢ã™
        news_candidates = soup.find_all(class_="box-title")
        print(f"ãƒœãƒƒã‚¯ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«å€™è£œ: {[t.text.strip() for t in news_candidates]}")

        # HTMLã®ä¸­èº«ã‚’å°‘ã—ã ã‘ãƒ­ã‚°ã«å‡ºã™ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        print("--- HTML DUMP (Head) ---")
        print(response.text[:1000]) # æœ€åˆã®1000æ–‡å­—ã ã‘

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")

if __name__ == "__main__":
    main()